# ğŸ“Š City Budget Query System

An end-to-end Retrieval-Augmented Generation (RAG) system that enables natural language querying of city budget PDFs using OpenAI's GPT models, vector search, and OCR-enhanced document parsing. Features a FastAPI backend, Flask-based frontend, and supports Pinecone or ChromaDB as the vector store.

---

## ğŸš€ Features

- âœ… Upload and parse scanned or digital PDFs with OCR fallback
- âœ… Extract text and tables with PyMuPDF, Tesseract, and Camelot
- âœ… Clean and chunk content using LangChain's RecursiveCharacterTextSplitter
- âœ… Generate embeddings using OpenAIâ€™s `text-embedding-3-small`
- âœ… Store and retrieve chunks with Pinecone or ChromaDB
- âœ… Run semantic search + prompt-augmented LLM answers
- âœ… Redis caching for repeat queries
- âœ… Full-stack: FastAPI (API), Flask (frontend), Docker-ready

---

## ğŸ§  Architecture Overview

```plaintext
PDFs â†’ PDFProcessor (OCR/Text/Table) 
     â†’ TextProcessor (clean/chunk)
     â†’ EmbeddingGenerator (OpenAI)
     â†’ VectorStore (Pinecone / ChromaDB)
     â†’ QueryEngine (OpenAI GPT)
     â†’ API Response

ğŸ§¾ Folder Structure
city-budget-query/
â”œâ”€â”€ api.py                 # FastAPI backend
â”œâ”€â”€ web/web_app.py         # Flask frontend
â”œâ”€â”€ main.py                # CLI runner / core pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py          # Loads env vars via dotenv
â”‚   â”œâ”€â”€ pdf_processor.py   # OCR + table extractor
â”‚   â”œâ”€â”€ text_processor.py  # Chunking and cleaning
â”‚   â”œâ”€â”€ embeddings.py      # OpenAI embedding calls
â”‚   â”œâ”€â”€ query_engine.py    # RAG core logic
â”‚   â””â”€â”€ vector_store.py    # Pinecone / Chroma support
â”œâ”€â”€ static/                # Serves HTML page
â”œâ”€â”€ templates/             # Flask template
â”œâ”€â”€ data/pdfs              # Uploaded budget PDFs
â”œâ”€â”€ data/processed         # Intermediate processed content
â”œâ”€â”€ docker-compose.yml     # Docker + Redis setup
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # API keys and settings
â””â”€â”€ README.md
```
## âš™ï¸ Environment Configuration (.env)
```
Create a .env file in the project root:
OPENAI_API_KEY=your-openai-key
PINECONE_API_KEY=your-pinecone-key
PINECONE_ENVIRONMENT=us-west1-gcp
REDIS_HOST=localhost
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=text-embedding-3-small  # Options: text-embedding-3-small, text-embedding-3-large
LLM_MODEL=gpt-4o-mini  # Options: gpt-4o-mini, gpt-4o, gpt-4-turbo
METADATA_EXTRACTION_MODEL=gpt-4o-mini  # Options: gpt-4o-mini, gpt-4o
VECTOR_DB_INDEX=city-budgets
```
## ğŸ³ Docker Setup
```
docker-compose up --build

API: http://localhost:8000

Frontend: http://localhost:5000

```
## ğŸ”§ Local Development
```
1. Set up Python environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

2. Run the FastAPI backend
uvicorn api:app --reload --port 8000

3. (Optional) Run the Flask web app
python web/web_app.py
```
## ğŸ§  Powered By
```
OpenAI GPT-4 Turbo

Pinecone / ChromaDB

Tesseract OCR

Camelot PDF Tables

FastAPI

Flask

Redis
```
## ğŸ™Œ Acknowledgments
```
This project was built to make city budgets more accessible, understandable, and transparent through the power of LLMs and open-source AI.
```
